{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mentoring Week 8 - Data Integration and ETL Pipeline** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Data Pipeline with Python and Pyspark - Pacmann AI ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a Data Engineer, we need to understand and assessing the quality of a given dataset containing sales data. This responsibilities include:\n",
    "\n",
    "1. **Data Profiling:** Explore the dataset to gain insights into its structure and attributes.\n",
    "\n",
    "2. **Data Quality Check:** Assess the validity and consistency of the data. Identify any anomalies or missing values.\n",
    "\n",
    "3. **Recommendations:** Based on your findings, provide recommendations for cleaning and improving the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Dataset**\n",
    "\n",
    "* Use Docker Compose to run the container: [repository](https://github.com/Kurikulum-Sekolah-Pacmann/data_pipeline_exercise_4)\n",
    "* This dataset provides detailed information about car sales and spread across multiple sources from the database, API, and Spreadsheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pipeline Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **~ Helper Function** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "from minio import Minio\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup Logging Configuration\n",
    "\n",
    "LOG_DIR = \"log\"\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"info_process.log\")\n",
    "\n",
    "# Ensure log directory exists\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "# Configure logging only once\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "# Database Connections\n",
    "def get_db_connection(db_type):\n",
    "    if db_type == 'source':\n",
    "        return create_engine(f\"postgresql://{os.getenv('SRC_POSTGRES_USER')}:{os.getenv('SRC_POSTGRES_PASSWORD')}@{os.getenv('SRC_POSTGRES_HOST')}:{os.getenv('SRC_POSTGRES_PORT')}/{os.getenv('SRC_POSTGRES_DB')}\")\n",
    "    elif db_type == 'staging':\n",
    "        return create_engine(f\"postgresql://{os.getenv('STG_POSTGRES_USER')}:{os.getenv('STG_POSTGRES_PASSWORD')}@{os.getenv('STG_POSTGRES_HOST')}:{os.getenv('STG_POSTGRES_PORT')}/{os.getenv('STG_POSTGRES_DB')}\")\n",
    "    elif db_type == 'warehouse':\n",
    "        return create_engine(f\"postgresql://{os.getenv('WH_POSTGRES_USER')}:{os.getenv('WH_POSTGRES_PASSWORD')}@{os.getenv('WH_POSTGRES_HOST')}:{os.getenv('WH_POSTGRES_PORT')}/{os.getenv('WH_POSTGRES_DB')}\")\n",
    "    elif db_type == 'log':\n",
    "        return create_engine(f\"postgresql://{os.getenv('LOG_POSTGRES_USER')}:{os.getenv('LOG_POSTGRES_PASSWORD')}@{os.getenv('LOG_POSTGRES_HOST')}:{os.getenv('LOG_POSTGRES_PORT')}/{os.getenv('LOG_POSTGRES_DB')}\")\n",
    "\n",
    "# Logging\n",
    "def etl_log(log_msg: dict):\n",
    "    # Write log to database\n",
    "    try:\n",
    "        conn = get_db_connection('log')\n",
    "        df_log = pd.DataFrame([log_msg])\n",
    "        df_log.to_sql(name=\"etl_log\", con=conn, if_exists=\"append\", index=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Can't save log to DB. Cause: {str(e)}\")\n",
    "\n",
    "    # Write log to file\n",
    "    try:\n",
    "        log_line = \"\"\n",
    "\n",
    "        for key, value in log_msg.items():\n",
    "            log_line += f\"{key}={value} | \"\n",
    "\n",
    "        log_line = log_line.rstrip(\" | \")\n",
    "\n",
    "        logging.info(log_line)\n",
    "    except Exception as e:\n",
    "        print(f\"Can't save log to file. Cause: {str(e)}\")\n",
    "\n",
    "def read_etl_log(filter_params: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the latest etl_date from the log table for incremental extraction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create connection to database        \n",
    "        conn = get_db_connection('log')\n",
    "\n",
    "        query = sqlalchemy.text(\"\"\"\n",
    "            SELECT MAX(etl_date) as latest_etl_date\n",
    "            FROM etl_log\n",
    "            WHERE \n",
    "                step = :step AND\n",
    "                component = :component AND\n",
    "                status = :status AND\n",
    "                table_name ILIKE :table_name\n",
    "        \"\"\")\n",
    "\n",
    "        # Execute the query with pd.read_sql\n",
    "        df = pd.read_sql(sql=query, con=conn, params=filter_params)\n",
    "\n",
    "        #return extracted data\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Can't execute your query. Cause: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Read SQL Query from Table\n",
    "def read_sql(table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a basic SQL query to select all rows from the specified table\n",
    "    \"\"\"\n",
    "    # // where created_at is greater than a given etl_date (parameterized) (WHERE created_at > :etl_date). \n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    return query\n",
    "\n",
    "def read_sql_inc(table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a basic SQL query to select all rows from the specified table\n",
    "    Where created_at is greater than a given etl_date (parameterized) (WHERE created_at > :etl_date). \n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {table_name} WHERE created_at > :etl_date\"\n",
    "    return query\n",
    "\n",
    "# Create Function handle_error to dump failure data to MiniO\n",
    "def handle_error(data, bucket_name: str, table_name: str, step: str, component: str):\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Initialize MinIO client\n",
    "    client = Minio('localhost:9000',\n",
    "                access_key=os.getenv('MINIO_ACCESS_KEY'),\n",
    "                secret_key=os.getenv('MINIO_SECRET_KEY'),\n",
    "                secure=False)\n",
    "    \n",
    "    # Make a bucket if it doesn't exist\n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "\n",
    "    # Convert DataFrame to CSV and then to bytes\n",
    "    csv_bytes = data.to_csv().encode('utf-8')\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    # Upload the CSV file to the bucket\n",
    "    client.put_object(\n",
    "        bucket_name=bucket_name,\n",
    "        object_name=f\"{step}_{component}_{table_name}_{current_date}.csv\",\n",
    "        data=csv_buffer,\n",
    "        length=len(csv_bytes),\n",
    "        content_type='application/csv'\n",
    "    )\n",
    "\n",
    "    # List objects in the bucket\n",
    "    objects = client.list_objects(bucket_name, recursive=True)\n",
    "    for obj in objects:\n",
    "        print(obj.object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **~ Data Profiling and Data Quality** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person_in_charge': 'Reza',\n",
       " 'date_profiling': '2025-04-20 14:14:13.176267',\n",
       " 'result': {'db_car_sales': {'shape': (30000, 17),\n",
       "   'data_types': {'id_sales': 'int64',\n",
       "    'year': 'int64',\n",
       "    'brand_car': 'object',\n",
       "    'model': 'object',\n",
       "    'trim': 'object',\n",
       "    'body': 'object',\n",
       "    'transmission': 'object',\n",
       "    'vin': 'object',\n",
       "    'state': 'object',\n",
       "    'condition': 'float64',\n",
       "    'odometer': 'float64',\n",
       "    'color': 'object',\n",
       "    'interior': 'object',\n",
       "    'seller': 'object',\n",
       "    'mmr': 'float64',\n",
       "    'sellingprice': 'float64',\n",
       "    'saledate': 'object'},\n",
       "   'unique_values': {'state': ['fl',\n",
       "     'mo',\n",
       "     'nj',\n",
       "     'pa',\n",
       "     'il',\n",
       "     'tx',\n",
       "     'ut',\n",
       "     'mn',\n",
       "     'ca',\n",
       "     'md',\n",
       "     'va',\n",
       "     'ga',\n",
       "     'qc',\n",
       "     'ma',\n",
       "     'mi',\n",
       "     'tn',\n",
       "     'nc',\n",
       "     'oh',\n",
       "     'ms',\n",
       "     'co',\n",
       "     'sc',\n",
       "     'wi',\n",
       "     'az',\n",
       "     'hi',\n",
       "     'ne',\n",
       "     'wa',\n",
       "     'ny',\n",
       "     'pr',\n",
       "     'nv',\n",
       "     'on',\n",
       "     'in',\n",
       "     'la',\n",
       "     'nm',\n",
       "     'ab',\n",
       "     'or',\n",
       "     'ok',\n",
       "     'ns',\n",
       "     '3vwd17aj5fm219943',\n",
       "     '3vwd17aj5fm297123'],\n",
       "    'body': ['Sedan',\n",
       "     'Quad Cab',\n",
       "     'Convertible',\n",
       "     'SUV',\n",
       "     'Van',\n",
       "     'sedan',\n",
       "     'suv',\n",
       "     'Hatchback',\n",
       "     'SuperCrew',\n",
       "     'coupe',\n",
       "     'Crew Cab',\n",
       "     'G Sedan',\n",
       "     'Regular Cab',\n",
       "     'Minivan',\n",
       "     'Coupe',\n",
       "     'wagon',\n",
       "     'Wagon',\n",
       "     'regular cab',\n",
       "     'SuperCab',\n",
       "     'G Coupe',\n",
       "     'crew cab',\n",
       "     'Extended Cab',\n",
       "     '',\n",
       "     'hatchback',\n",
       "     'convertible',\n",
       "     'extended cab',\n",
       "     'minivan',\n",
       "     'supercrew',\n",
       "     'E-Series Van',\n",
       "     'Q60 Convertible',\n",
       "     'q60 coupe',\n",
       "     'quad cab',\n",
       "     'Double Cab',\n",
       "     'cts coupe',\n",
       "     'Elantra Coupe',\n",
       "     'Cab Plus',\n",
       "     'supercab',\n",
       "     'G Convertible',\n",
       "     'e-series van',\n",
       "     'g sedan',\n",
       "     'King Cab',\n",
       "     'king cab',\n",
       "     'crewmax cab',\n",
       "     'Genesis Coupe',\n",
       "     'Access Cab',\n",
       "     'genesis coupe',\n",
       "     'Koup',\n",
       "     'van',\n",
       "     'Beetle Convertible',\n",
       "     'g coupe',\n",
       "     'Club Cab',\n",
       "     'CrewMax Cab',\n",
       "     'double cab',\n",
       "     'CTS Coupe',\n",
       "     'tsx sport wagon',\n",
       "     'club cab',\n",
       "     'Transit Van',\n",
       "     'Navitgation',\n",
       "     'Q60 Coupe',\n",
       "     'mega cab',\n",
       "     'access cab',\n",
       "     'g convertible',\n",
       "     'koup',\n",
       "     'Xtracab',\n",
       "     'cts-v coupe',\n",
       "     'Mega Cab',\n",
       "     'Promaster Cargo Van',\n",
       "     'TSX Sport Wagon',\n",
       "     'CTS-V Coupe',\n",
       "     'elantra coupe'],\n",
       "    'color': ['white',\n",
       "     '—',\n",
       "     'red',\n",
       "     'black',\n",
       "     'burgundy',\n",
       "     'blue',\n",
       "     'silver',\n",
       "     'purple',\n",
       "     '',\n",
       "     'gray',\n",
       "     'gold',\n",
       "     'beige',\n",
       "     'green',\n",
       "     'charcoal',\n",
       "     'yellow',\n",
       "     'brown',\n",
       "     'orange',\n",
       "     'off-white',\n",
       "     'turquoise',\n",
       "     '16633',\n",
       "     'pink',\n",
       "     '6388'],\n",
       "    'interior': ['black',\n",
       "     'beige',\n",
       "     'brown',\n",
       "     'tan',\n",
       "     'gray',\n",
       "     '',\n",
       "     '—',\n",
       "     'silver',\n",
       "     'green',\n",
       "     'off-white',\n",
       "     'blue',\n",
       "     'red',\n",
       "     'burgundy',\n",
       "     'gold',\n",
       "     'purple',\n",
       "     'orange',\n",
       "     'white']},\n",
       "   'missing_percentage': {'id_sales': 0.0,\n",
       "    'year': 0.0,\n",
       "    'brand_car': 1.75,\n",
       "    'model': 1.77,\n",
       "    'trim': 1.84,\n",
       "    'body': 2.31,\n",
       "    'transmission': 11.87,\n",
       "    'vin': 0.0,\n",
       "    'state': 0.0,\n",
       "    'condition': 2.2,\n",
       "    'odometer': 0.02,\n",
       "    'color': 4.65,\n",
       "    'interior': 3.48,\n",
       "    'seller': 0.0,\n",
       "    'mmr': 0.01,\n",
       "    'sellingprice': 0.0,\n",
       "    'saledate': 0.0}},\n",
       "  'api_us_state': {'shape': (68, 3),\n",
       "   'data_types': {'id_state': 'int64', 'code': 'object', 'name': 'object'},\n",
       "   'unique_values': {},\n",
       "   'missing_percentage': {'id_state': 0.0, 'code': 0.0, 'name': 0.0}},\n",
       "  'sheet_brand_car': {'shape': (51, 3),\n",
       "   'data_types': {'brand_car_id': 'object',\n",
       "    'brand_name': 'object',\n",
       "    'created_at': 'object'},\n",
       "   'unique_values': {},\n",
       "   'missing_percentage': {'brand_car_id': 0.0,\n",
       "    'brand_name': 0.0,\n",
       "    'created_at': 0.0}}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from helper.utils import etl_log\n",
    "from staging.extract.extract_api import extract_api\n",
    "from staging.extract.extract_spreadsheet import extract_sheet\n",
    "from staging.extract.extract_db import extract_database\n",
    "\n",
    "def extract_all_sources():\n",
    "    \"\"\"Extract data from all sources and return combined dictionary.\"\"\"\n",
    "    combined_data = {}\n",
    "\n",
    "    # Extract from database\n",
    "    df_car_sales = extract_database(table_name=\"car_sales\")\n",
    "    combined_data[\"db_car_sales\"] = df_car_sales\n",
    "\n",
    "    # Extract from API\n",
    "    df_api = extract_api(\n",
    "        link_api=\"https://raw.githubusercontent.com/Kurikulum-Sekolah-Pacmann/us_states_data/refs/heads/main/us_states.json\",\n",
    "        list_parameter={},\n",
    "        data_name=\"regions\"\n",
    "    )\n",
    "    combined_data[\"api_us_state\"] = df_api\n",
    "\n",
    "    # Extract from Spreadsheet\n",
    "    spreadsheet_key = os.getenv(\"KEY_SPREADSHEET\")\n",
    "    worksheet_name = \"brand_car\"\n",
    "    df_sheet = extract_sheet(spreadsheet_key, worksheet_name)\n",
    "    combined_data[\"sheet_brand_car\"] = df_sheet\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def table_shapes(data):\n",
    "    return {table: df.shape for table, df in data.items()}\n",
    "\n",
    "def column_types(data):\n",
    "    return {table: {col: str(df[col].dtype) for col in df.columns} for table, df in data.items()}\n",
    "\n",
    "def unique_values(data):\n",
    "    target_columns = ['state', 'body', 'color', 'interior']\n",
    "    result = {}\n",
    "\n",
    "    # Only check unique values for the database data (db_car_sales)\n",
    "    if \"db_car_sales\" in data:\n",
    "        df = data[\"db_car_sales\"]\n",
    "        result[\"db_car_sales\"] = {}\n",
    "        for col in target_columns:\n",
    "            result[\"db_car_sales\"][col] = df[col].unique().tolist() if col in df.columns else []\n",
    "    return result\n",
    "\n",
    "def missing_value_percent(data):\n",
    "    result = {}\n",
    "    for table, df in data.items():\n",
    "        result[table] = {}\n",
    "        for col in df.columns:\n",
    "            # Include '' and '—' as missing value\n",
    "            missing_mask = df[col].isnull() | (df[col] == '') | (df[col] == '—')\n",
    "            missing_percentage = round(float(missing_mask.mean() * 100), 2)\n",
    "            result[table][col] = missing_percentage\n",
    "    return result\n",
    "\n",
    "\n",
    "def profile_report():\n",
    "    data = extract_all_sources()\n",
    "\n",
    "    report = {\n",
    "        \"person_in_charge\": \"Reza\",\n",
    "        \"date_profiling\": str(datetime.now()),\n",
    "        \"result\": {}\n",
    "    }\n",
    "\n",
    "    shape_result = table_shapes(data)\n",
    "    type_result = column_types(data)\n",
    "    unique_result = unique_values(data)\n",
    "    missing_result = missing_value_percent(data)\n",
    "\n",
    "    for table in data.keys():\n",
    "        report[\"result\"][table] = {\n",
    "            \"shape\": shape_result[table],\n",
    "            \"data_types\": type_result[table],\n",
    "            \"unique_values\": unique_result.get(table, {}),\n",
    "            \"missing_percentage\": missing_result[table]\n",
    "        }\n",
    "\n",
    "    # Save JSON\n",
    "    os.makedirs(\"profiling/output_profiling\", exist_ok=True)\n",
    "    output_path = os.path.join(\"profiling/output_profiling\", \"car_sales_profiling_report.json\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    return report\n",
    "\n",
    "# Run it\n",
    "profile_report()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. STAGING** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STAGING_Extract from API** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from helper.utils import etl_log\n",
    "\n",
    "def extract_api(link_api:str, list_parameter:dict, data_name:str) -> pd.DataFrame:\n",
    "    log_msg = {\n",
    "        \"step\": \"staging\",\n",
    "        \"component\": \"extract_api\",\n",
    "        \"table_name\": data_name,\n",
    "        \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Establish connection to API        \n",
    "        resp = requests.get(link_api, params=list_parameter)\n",
    "        resp.raise_for_status()  # Raises error if status is not 200\n",
    "\n",
    "        # Parse the response JSON\n",
    "        raw_response = resp.json()\n",
    "\n",
    "        # Convert the JSON data to a pandas DataFrame        \n",
    "        df_api = pd.DataFrame(raw_response)\n",
    "\n",
    "        # Convert the key into a list and return it as a DataFrame\n",
    "        df_result = pd.DataFrame(df_api[data_name].tolist())\n",
    "\n",
    "        # create success log message\n",
    "        log_msg[\"status\"] = \"success\"\n",
    "        return df_result\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # create fail log message        \n",
    "        print(f\"API request error: {e}\")\n",
    "        log_msg[\"status\"] = \"failed\"\n",
    "        log_msg[\"error_msg\"] = str(e)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    except ValueError as e:\n",
    "        # create fail log message        \n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        log_msg[\"status\"] = \"failed\"\n",
    "        log_msg[\"error_msg\"] = str(e)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api = extract_api(link_api=\"https://raw.githubusercontent.com/Kurikulum-Sekolah-Pacmann/us_states_data/refs/heads/main/us_states.json\", list_parameter=\"\", data_name=\"regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_state</th>\n",
       "      <th>code</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>al</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ak</td>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>az</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ar</td>\n",
       "      <td>Arkansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ca</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>pr</td>\n",
       "      <td>Puerto Rico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>vi</td>\n",
       "      <td>U.S. Virgin Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>gu</td>\n",
       "      <td>Guam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>mp</td>\n",
       "      <td>Northern Mariana Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>as</td>\n",
       "      <td>American Samoa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_state code                      name\n",
       "0          1   al                   Alabama\n",
       "1          2   ak                    Alaska\n",
       "2          3   az                   Arizona\n",
       "3          4   ar                  Arkansas\n",
       "4          5   ca                California\n",
       "..       ...  ...                       ...\n",
       "63        64   pr               Puerto Rico\n",
       "64        65   vi       U.S. Virgin Islands\n",
       "65        66   gu                      Guam\n",
       "66        67   mp  Northern Mariana Islands\n",
       "67        68   as            American Samoa\n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STAGING_Extract from Database** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import text \n",
    "import sqlalchemy\n",
    "from helper.utils import get_db_connection, etl_log, read_etl_log, read_sql\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_database(table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts data from the source database incrementally.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection('source')\n",
    "        \n",
    "        # Get the latest etl_date from the log table\n",
    "        filter_log = {\n",
    "            \"step\": \"staging\",\n",
    "            \"component\": \"load\",\n",
    "            \"status\": \"success\",\n",
    "            \"table_name\": table_name\n",
    "        }\n",
    "        etl_date = read_etl_log(filter_log)\n",
    "\n",
    "        # If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "        # Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "        # Set etl_date for incremental extraction\n",
    "        if etl_date.empty or etl_date['latest_etl_date'][0] is None:\n",
    "            etl_date = '1111-01-01'\n",
    "        else:\n",
    "            etl_date = etl_date['latest_etl_date'][0]\n",
    "\n",
    "        # Constructs a SQL query to select all columns from the specified table_name\n",
    "        #  where created_at is greater than etl_date.\n",
    "        \"\"\"\n",
    "        SELECT * \n",
    "        FROM car_sales \n",
    "        \"\"\"\n",
    "        # WHERE created_at > :etl_date\n",
    "        query = sqlalchemy.text(read_sql(table_name))\n",
    "        # df = pd.read_sql(sql=query, con=conn, params={\"etl_date\": etl_date})\n",
    "        df = pd.read_sql(sql=query, con=conn, params={})\n",
    "\n",
    "        # Log success\n",
    "        log_msg = {\n",
    "            \"step\": \"staging\",\n",
    "            \"component\": \"extract_database\",\n",
    "            \"status\": \"success\",\n",
    "            # \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        log_msg = {\n",
    "            \"step\": \"staging\",\n",
    "            \"component\": \"extract_database\",\n",
    "            \"status\": \"failed\",\n",
    "            # \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        print(e)\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sales</th>\n",
       "      <th>year</th>\n",
       "      <th>brand_car</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body</th>\n",
       "      <th>transmission</th>\n",
       "      <th>vin</th>\n",
       "      <th>state</th>\n",
       "      <th>condition</th>\n",
       "      <th>odometer</th>\n",
       "      <th>color</th>\n",
       "      <th>interior</th>\n",
       "      <th>seller</th>\n",
       "      <th>mmr</th>\n",
       "      <th>sellingprice</th>\n",
       "      <th>saledate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Impala Limited</td>\n",
       "      <td>LT Fleet</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>automatic</td>\n",
       "      <td>2g1wb5e37e1112559</td>\n",
       "      <td>fl</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21507.0</td>\n",
       "      <td>white</td>\n",
       "      <td>black</td>\n",
       "      <td>gm remarketing</td>\n",
       "      <td>13450.0</td>\n",
       "      <td>13800.0</td>\n",
       "      <td>Mon Feb 23 2015 05:00:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2003</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>Ram Pickup 1500</td>\n",
       "      <td>SLT</td>\n",
       "      <td>Quad Cab</td>\n",
       "      <td></td>\n",
       "      <td>1d7ha18n13s152972</td>\n",
       "      <td>mo</td>\n",
       "      <td>31.0</td>\n",
       "      <td>79712.0</td>\n",
       "      <td>—</td>\n",
       "      <td>black</td>\n",
       "      <td>tdaf remarketing</td>\n",
       "      <td>6025.0</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>Tue Jan 20 2015 02:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>Pontiac</td>\n",
       "      <td>G6</td>\n",
       "      <td>GT</td>\n",
       "      <td>Convertible</td>\n",
       "      <td>automatic</td>\n",
       "      <td>1g2zh361474252178</td>\n",
       "      <td>nj</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65698.0</td>\n",
       "      <td>red</td>\n",
       "      <td>black</td>\n",
       "      <td>car authority inc</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>Wed Jan 14 2015 01:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Corolla</td>\n",
       "      <td>LE</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>automatic</td>\n",
       "      <td>jtdbu4eexb9167571</td>\n",
       "      <td>fl</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23634.0</td>\n",
       "      <td>black</td>\n",
       "      <td>beige</td>\n",
       "      <td>world omni financial corporation</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>11400.0</td>\n",
       "      <td>Tue Jan 27 2015 01:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2012</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>ES 350</td>\n",
       "      <td>Base</td>\n",
       "      <td>Sedan</td>\n",
       "      <td></td>\n",
       "      <td>jthbk1eg6c2495519</td>\n",
       "      <td>pa</td>\n",
       "      <td>35.0</td>\n",
       "      <td>26483.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>meridian remarketing</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>23300.0</td>\n",
       "      <td>Fri Jan 30 2015 01:00:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29996</td>\n",
       "      <td>2012</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Explorer</td>\n",
       "      <td>XLT</td>\n",
       "      <td>SUV</td>\n",
       "      <td>automatic</td>\n",
       "      <td>1fmhk8d83cga41998</td>\n",
       "      <td>mi</td>\n",
       "      <td>36.0</td>\n",
       "      <td>103016.0</td>\n",
       "      <td>white</td>\n",
       "      <td>gray</td>\n",
       "      <td>automobiles paille inc</td>\n",
       "      <td>17700.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>Thu Jan 29 2015 01:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29997</td>\n",
       "      <td>2012</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Jetta</td>\n",
       "      <td>Base</td>\n",
       "      <td>sedan</td>\n",
       "      <td>automatic</td>\n",
       "      <td>3vw2k7aj1cm312846</td>\n",
       "      <td>fl</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41092.0</td>\n",
       "      <td>—</td>\n",
       "      <td>tan</td>\n",
       "      <td>vw credit</td>\n",
       "      <td>8475.0</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>Wed Jun 10 2015 02:40:00 GMT-0700 (PDT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29998</td>\n",
       "      <td>2003</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Tacoma</td>\n",
       "      <td>Base</td>\n",
       "      <td>Regular Cab</td>\n",
       "      <td></td>\n",
       "      <td>5tenl42n03z286594</td>\n",
       "      <td>az</td>\n",
       "      <td>19.0</td>\n",
       "      <td>292925.0</td>\n",
       "      <td>white</td>\n",
       "      <td>gray</td>\n",
       "      <td>ge fleet services for itself/servicer</td>\n",
       "      <td>3225.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>Thu May 21 2015 05:00:00 GMT-0700 (PDT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29999</td>\n",
       "      <td>2014</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Impala Limited</td>\n",
       "      <td>LT Fleet</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>automatic</td>\n",
       "      <td>2g1wb5e37e1147702</td>\n",
       "      <td>fl</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25083.0</td>\n",
       "      <td>gray</td>\n",
       "      <td>gray</td>\n",
       "      <td>gm remarketing</td>\n",
       "      <td>12900.0</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>Mon Jan 26 2015 05:00:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>30000</td>\n",
       "      <td>2012</td>\n",
       "      <td>RAM</td>\n",
       "      <td>1500</td>\n",
       "      <td>Laramie Longhorn Edition</td>\n",
       "      <td>Crew Cab</td>\n",
       "      <td></td>\n",
       "      <td>1c6rd7pt4cs303383</td>\n",
       "      <td>pa</td>\n",
       "      <td>41.0</td>\n",
       "      <td>45750.0</td>\n",
       "      <td>black</td>\n",
       "      <td>black</td>\n",
       "      <td>j magnone auto group llc</td>\n",
       "      <td>29700.0</td>\n",
       "      <td>31700.0</td>\n",
       "      <td>Fri Jan 30 2015 01:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_sales  year   brand_car            model                      trim  \\\n",
       "0             1  2014   Chevrolet   Impala Limited                  LT Fleet   \n",
       "1             2  2003       Dodge  Ram Pickup 1500                       SLT   \n",
       "2             3  2007     Pontiac               G6                        GT   \n",
       "3             4  2011      Toyota          Corolla                        LE   \n",
       "4             5  2012       Lexus           ES 350                      Base   \n",
       "...         ...   ...         ...              ...                       ...   \n",
       "29995     29996  2012        Ford         Explorer                       XLT   \n",
       "29996     29997  2012  Volkswagen            Jetta                      Base   \n",
       "29997     29998  2003      Toyota           Tacoma                      Base   \n",
       "29998     29999  2014   Chevrolet   Impala Limited                  LT Fleet   \n",
       "29999     30000  2012         RAM             1500  Laramie Longhorn Edition   \n",
       "\n",
       "              body transmission                vin state  condition  odometer  \\\n",
       "0            Sedan    automatic  2g1wb5e37e1112559    fl        4.0   21507.0   \n",
       "1         Quad Cab               1d7ha18n13s152972    mo       31.0   79712.0   \n",
       "2      Convertible    automatic  1g2zh361474252178    nj       34.0   65698.0   \n",
       "3            Sedan    automatic  jtdbu4eexb9167571    fl       43.0   23634.0   \n",
       "4            Sedan               jthbk1eg6c2495519    pa       35.0   26483.0   \n",
       "...            ...          ...                ...   ...        ...       ...   \n",
       "29995          SUV    automatic  1fmhk8d83cga41998    mi       36.0  103016.0   \n",
       "29996        sedan    automatic  3vw2k7aj1cm312846    fl       36.0   41092.0   \n",
       "29997  Regular Cab               5tenl42n03z286594    az       19.0  292925.0   \n",
       "29998        Sedan    automatic  2g1wb5e37e1147702    fl        4.0   25083.0   \n",
       "29999     Crew Cab               1c6rd7pt4cs303383    pa       41.0   45750.0   \n",
       "\n",
       "       color interior                                 seller      mmr  \\\n",
       "0      white    black                         gm remarketing  13450.0   \n",
       "1          —    black                       tdaf remarketing   6025.0   \n",
       "2        red    black                      car authority inc   7375.0   \n",
       "3      black    beige       world omni financial corporation  10800.0   \n",
       "4      black    brown                   meridian remarketing  22500.0   \n",
       "...      ...      ...                                    ...      ...   \n",
       "29995  white     gray                 automobiles paille inc  17700.0   \n",
       "29996      —      tan                              vw credit   8475.0   \n",
       "29997  white     gray  ge fleet services for itself/servicer   3225.0   \n",
       "29998   gray     gray                         gm remarketing  12900.0   \n",
       "29999  black    black               j magnone auto group llc  29700.0   \n",
       "\n",
       "       sellingprice                                 saledate  \n",
       "0           13800.0  Mon Feb 23 2015 05:00:00 GMT-0800 (PST)  \n",
       "1            6300.0  Tue Jan 20 2015 02:30:00 GMT-0800 (PST)  \n",
       "2            8000.0  Wed Jan 14 2015 01:30:00 GMT-0800 (PST)  \n",
       "3           11400.0  Tue Jan 27 2015 01:30:00 GMT-0800 (PST)  \n",
       "4           23300.0  Fri Jan 30 2015 01:00:00 GMT-0800 (PST)  \n",
       "...             ...                                      ...  \n",
       "29995       16800.0  Thu Jan 29 2015 01:30:00 GMT-0800 (PST)  \n",
       "29996        9800.0  Wed Jun 10 2015 02:40:00 GMT-0700 (PDT)  \n",
       "29997        2400.0  Thu May 21 2015 05:00:00 GMT-0700 (PDT)  \n",
       "29998       12800.0  Mon Jan 26 2015 05:00:00 GMT-0800 (PST)  \n",
       "29999       31700.0  Fri Jan 30 2015 01:30:00 GMT-0800 (PST)  \n",
       "\n",
       "[30000 rows x 17 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STAGING_Extract from Google Spreadsheet** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helper.utils import etl_log\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "from google.auth.transport.requests import Request\n",
    "from google.auth import load_credentials_from_file\n",
    "import os\n",
    "\n",
    "def auth_gspread():\n",
    "    \"\"\"\n",
    "    Authenticates with Google Sheets API.\n",
    "    \"\"\"\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    \n",
    "    #Define your credentials\n",
    "    credentials, project = load_credentials_from_file((os.getenv('CRED_PATH')), scopes=scope)\n",
    "    return gspread.authorize(credentials)\n",
    "\n",
    "def init_key_file(key_file:str):\n",
    "    #define credentials to open the file\n",
    "    gc = auth_gspread()\n",
    "    \n",
    "    #open spreadsheet file by key\n",
    "    sheet_result = gc.open_by_key(key_file)\n",
    "    \n",
    "    return sheet_result\n",
    "\n",
    "def extract_sheet(key_file: str, worksheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts data from a Google Sheet.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # init sheet\n",
    "        sheet_result = init_key_file(key_file)\n",
    "        \n",
    "        worksheet_result = sheet_result.worksheet(worksheet_name)\n",
    "        \n",
    "        df_result = pd.DataFrame(worksheet_result.get_all_values())\n",
    "        \n",
    "        # set first rows as columns\n",
    "        df_result.columns = df_result.iloc[0]\n",
    "        \n",
    "        # get all the rest of the values\n",
    "        df_result = df_result[1:].copy()\n",
    "\n",
    "        # Add the 'created_at' column with the current datetime\n",
    "        df_result['created_at'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Log success\n",
    "        log_msg = {\n",
    "            \"step\": \"staging\",\n",
    "            \"component\": \"extract_spreadsheet\",\n",
    "            \"status\": \"success\",\n",
    "            # \"source\": \"spreadsheet\",\n",
    "            \"table_name\": worksheet_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        return df_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        log_msg = {\n",
    "            \"step\": \"staging\",\n",
    "            \"component\": \"extract_spreadsheet\",\n",
    "            \"status\": \"failed\",\n",
    "            # \"source\": \"spreadsheet\",\n",
    "            \"table_name\": worksheet_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        print(e)\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_car_id</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Acura</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Audi</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bentley</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BMW</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Buick</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Cadillac</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Chrysler</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Daewoo</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Ferrari</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>FIAT</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Ford</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Ford Truck</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Geo</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>GMC</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Honda</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Hummer</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Hyundai</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>Infiniti</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Isuzu</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Jaguar</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Kia</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Land Rover</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Landrover</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Maserati</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>Mazda</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>Mercury</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>Mini</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>Mitsubishi</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>Oldsmobile</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>Plymouth</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>Pontiac</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>Porsche</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>RAM</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>Rolls-Royce</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>Saab</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>Saturn</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>Scion</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>Smart</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>Subaru</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>Suzuki</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>Volvo</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>VW</td>\n",
       "      <td>2025-04-18 01:11:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0  brand_car_id   brand_name           created_at\n",
       "1             1        Acura  2025-04-18 01:11:54\n",
       "2             2         Audi  2025-04-18 01:11:54\n",
       "3             3      Bentley  2025-04-18 01:11:54\n",
       "4             4          BMW  2025-04-18 01:11:54\n",
       "5             5        Buick  2025-04-18 01:11:54\n",
       "6             6     Cadillac  2025-04-18 01:11:54\n",
       "7             7    Chevrolet  2025-04-18 01:11:54\n",
       "8             8     Chrysler  2025-04-18 01:11:54\n",
       "9             9       Daewoo  2025-04-18 01:11:54\n",
       "10           10        Dodge  2025-04-18 01:11:54\n",
       "11           11      Ferrari  2025-04-18 01:11:54\n",
       "12           12         FIAT  2025-04-18 01:11:54\n",
       "13           13         Ford  2025-04-18 01:11:54\n",
       "14           14   Ford Truck  2025-04-18 01:11:54\n",
       "15           15          Geo  2025-04-18 01:11:54\n",
       "16           16          GMC  2025-04-18 01:11:54\n",
       "17           17        Honda  2025-04-18 01:11:54\n",
       "18           18       Hummer  2025-04-18 01:11:54\n",
       "19           19      Hyundai  2025-04-18 01:11:54\n",
       "20           20     Infiniti  2025-04-18 01:11:54\n",
       "21           21        Isuzu  2025-04-18 01:11:54\n",
       "22           22       Jaguar  2025-04-18 01:11:54\n",
       "23           23         Jeep  2025-04-18 01:11:54\n",
       "24           24          Kia  2025-04-18 01:11:54\n",
       "25           25   Land Rover  2025-04-18 01:11:54\n",
       "26           26    Landrover  2025-04-18 01:11:54\n",
       "27           27        Lexus  2025-04-18 01:11:54\n",
       "28           28      Lincoln  2025-04-18 01:11:54\n",
       "29           29     Maserati  2025-04-18 01:11:54\n",
       "30           30        Mazda  2025-04-18 01:11:54\n",
       "31           31     Mercedes  2025-04-18 01:11:54\n",
       "32           32      Mercury  2025-04-18 01:11:54\n",
       "33           33         Mini  2025-04-18 01:11:54\n",
       "34           34   Mitsubishi  2025-04-18 01:11:54\n",
       "35           35       Nissan  2025-04-18 01:11:54\n",
       "36           36   Oldsmobile  2025-04-18 01:11:54\n",
       "37           37     Plymouth  2025-04-18 01:11:54\n",
       "38           38      Pontiac  2025-04-18 01:11:54\n",
       "39           39      Porsche  2025-04-18 01:11:54\n",
       "40           40          RAM  2025-04-18 01:11:54\n",
       "41           41  Rolls-Royce  2025-04-18 01:11:54\n",
       "42           42         Saab  2025-04-18 01:11:54\n",
       "43           43       Saturn  2025-04-18 01:11:54\n",
       "44           44        Scion  2025-04-18 01:11:54\n",
       "45           45        Smart  2025-04-18 01:11:54\n",
       "46           46       Subaru  2025-04-18 01:11:54\n",
       "47           47       Suzuki  2025-04-18 01:11:54\n",
       "48           48       Toyota  2025-04-18 01:11:54\n",
       "49           49   Volkswagen  2025-04-18 01:11:54\n",
       "50           50        Volvo  2025-04-18 01:11:54\n",
       "51           51           VW  2025-04-18 01:11:54"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_sheet(key_file=os.getenv('KEY_SPREADSHEET'), worksheet_name=\"brand_car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STAGING_Transform Datatype Car Sales** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transform_datatype_car_sales(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms car_sales data according to the source-to-target mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the columns to keep\n",
    "    selected_columns = [\n",
    "        \"id_sales\", \"year\", \"brand_car\", \"transmission\", \"state\",\n",
    "        \"condition\", \"odometer\", \"color\", \"interior\", \"mmr\", \"sellingprice\"\n",
    "    ]\n",
    "\n",
    "    # Select only the necessary columns\n",
    "    df = df[selected_columns].copy()\n",
    "\n",
    "    # Apply transformations\n",
    "    df[\"year\"] = df[\"year\"].astype(str)\n",
    "    df[\"condition\"] = df[\"condition\"].astype(str)\n",
    "    df[\"odometer\"] = df[\"odometer\"].astype(str)\n",
    "    df[\"mmr\"] = df[\"mmr\"].astype(str)\n",
    "    df[\"sellingprice\"] = df[\"sellingprice\"].astype(str)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df_car_sales = transform_datatype_car_sales(df_car_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_staging(data=tf_df_car_sales, schema='public', table_name='car_sales', idx_name='id_sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STAGING_Load Staging** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helper.utils import get_db_connection, etl_log, handle_error\n",
    "from datetime import datetime\n",
    "from pangres import upsert\n",
    "\n",
    "def load_staging(data, schema: str, table_name: str, idx_name: str):\n",
    "    try:\n",
    "        conn = get_db_connection('staging')\n",
    "        data = data.set_index(idx_name)\n",
    "\n",
    "        # Do upsert (Update for existing data and Insert for new data)\n",
    "        upsert(con=conn, \n",
    "               df=data, \n",
    "               table_name=table_name, \n",
    "               schema=schema, \n",
    "               if_row_exists=\"update\")\n",
    "        \n",
    "        #create success log message\n",
    "        log_msg = {\n",
    "            \"step\": \"staging\",\n",
    "            \"component\": \"load\",\n",
    "            \"status\": \"success\",\n",
    "            # \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\": \"staging\",\n",
    "            \"component\": \"load\",\n",
    "            \"status\": \"failed\",\n",
    "            # \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        \n",
    "        # Handling error: save data to Object Storage\n",
    "        # try:\n",
    "        #     handle_error(data = data, bucket_name='error-paccar', table_name= table_name, step='staging', component='load')\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STAGING PIPELINE** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df_car_sales = extract_database(table_name=\"car_sales\")\n",
    "\n",
    "    # Extract data from api\n",
    "    df_us_state = extract_api(link_api=\"https://raw.githubusercontent.com/Kurikulum-Sekolah-Pacmann/us_states_data/refs/heads/main/us_states.json\", list_parameter=\"\", data_name=\"regions\")\n",
    "\n",
    "    # Extract data from spreadsheet\n",
    "    df_car_brand = extract_sheet(key_file=os.getenv('KEY_SPREADSHEET'), worksheet_name=\"brand_car\")\n",
    "\n",
    "    # Transform car sales data from database\n",
    "    tf_df_car_sales = transform_car_sales(df_car_sales)\n",
    "    \n",
    "    # Load data into staging (except last column, created_at)\n",
    "    load_staging(data=tf_df_car_sales, schema='public', table_name='car_sales', idx_name='id_sales')\n",
    "    load_staging(data=df_us_state, schema='public', table_name='us_state', idx_name='id_state')\n",
    "    load_staging(data=df_car_brand, schema='public', table_name='car_brand', idx_name='brand_car_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. WAREHOUSE** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WAREHOUSE_Extract from Staging** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import text \n",
    "import sqlalchemy\n",
    "from helper.utils import get_db_connection, etl_log, read_etl_log, read_sql_inc\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_staging(table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts data from the staging database incrementally.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection('staging')\n",
    "        \n",
    "        # Get the latest etl_date from the log table\n",
    "        filter_log = {\n",
    "            \"step\": \"warehouse\",\n",
    "            \"table_name\": table_name,\n",
    "            \"status\": \"success\",\n",
    "            \"component\": \"load\"\n",
    "        }\n",
    "        etl_date = read_etl_log(filter_log)\n",
    "\n",
    "        # If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "        # Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "        # Set etl_date for incremental extraction\n",
    "        if etl_date.empty or etl_date['latest_etl_date'][0] is None:\n",
    "            etl_date = '1111-01-01'\n",
    "        else:\n",
    "            etl_date = etl_date['latest_etl_date'][0]\n",
    "\n",
    "        # Constructs a SQL query to select all columns from the specified table_name where created_at is greater than etl_date.\n",
    "        \"\"\"\n",
    "        SELECT * \n",
    "        FROM car_sales \n",
    "        WHERE created_at > :etl_date\n",
    "        \"\"\"\n",
    "        query = sqlalchemy.text(read_sql_inc(table_name))\n",
    "        df = pd.read_sql(sql=query, con=conn, params={\"etl_date\": etl_date})\n",
    "\n",
    "        # Log success\n",
    "        log_msg = {\n",
    "            \"step\": \"warehouse\",\n",
    "            \"component\": \"extract\",\n",
    "            \"status\": \"success\",\n",
    "            # \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        log_msg = {\n",
    "            \"step\": \"warehouse\",\n",
    "            \"component\": \"extract\",\n",
    "            \"status\": \"failed\",\n",
    "            # \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        print(e)\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sales</th>\n",
       "      <th>year</th>\n",
       "      <th>brand_car</th>\n",
       "      <th>transmission</th>\n",
       "      <th>state</th>\n",
       "      <th>condition</th>\n",
       "      <th>odometer</th>\n",
       "      <th>color</th>\n",
       "      <th>interior</th>\n",
       "      <th>mmr</th>\n",
       "      <th>sellingprice</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400</td>\n",
       "      <td>2009</td>\n",
       "      <td>BMW</td>\n",
       "      <td></td>\n",
       "      <td>pa</td>\n",
       "      <td>33.0</td>\n",
       "      <td>58609.0</td>\n",
       "      <td>blue</td>\n",
       "      <td>tan</td>\n",
       "      <td>17250.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>2012</td>\n",
       "      <td>Ford</td>\n",
       "      <td>automatic</td>\n",
       "      <td>il</td>\n",
       "      <td>32.0</td>\n",
       "      <td>39151.0</td>\n",
       "      <td>red</td>\n",
       "      <td>tan</td>\n",
       "      <td>9525.0</td>\n",
       "      <td>9700.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2514</td>\n",
       "      <td>2003</td>\n",
       "      <td>Toyota</td>\n",
       "      <td></td>\n",
       "      <td>nc</td>\n",
       "      <td>36.0</td>\n",
       "      <td>83638.0</td>\n",
       "      <td>black</td>\n",
       "      <td>grey</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4469</td>\n",
       "      <td>2006</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nj</td>\n",
       "      <td>36.0</td>\n",
       "      <td>59574.0</td>\n",
       "      <td>white</td>\n",
       "      <td>black</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4957</td>\n",
       "      <td>2000</td>\n",
       "      <td>Ford</td>\n",
       "      <td></td>\n",
       "      <td>va</td>\n",
       "      <td>nan</td>\n",
       "      <td>88098.0</td>\n",
       "      <td>black</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29996</td>\n",
       "      <td>2012</td>\n",
       "      <td>Ford</td>\n",
       "      <td>automatic</td>\n",
       "      <td>mi</td>\n",
       "      <td>36.0</td>\n",
       "      <td>103016.0</td>\n",
       "      <td>white</td>\n",
       "      <td>grey</td>\n",
       "      <td>17700.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29997</td>\n",
       "      <td>2012</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>automatic</td>\n",
       "      <td>fl</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41092.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>tan</td>\n",
       "      <td>8475.0</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29998</td>\n",
       "      <td>2003</td>\n",
       "      <td>Toyota</td>\n",
       "      <td></td>\n",
       "      <td>az</td>\n",
       "      <td>19.0</td>\n",
       "      <td>292925.0</td>\n",
       "      <td>white</td>\n",
       "      <td>grey</td>\n",
       "      <td>3225.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29999</td>\n",
       "      <td>2014</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>automatic</td>\n",
       "      <td>fl</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25083.0</td>\n",
       "      <td>grey</td>\n",
       "      <td>grey</td>\n",
       "      <td>12900.0</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>30000</td>\n",
       "      <td>2012</td>\n",
       "      <td>RAM</td>\n",
       "      <td></td>\n",
       "      <td>pa</td>\n",
       "      <td>41.0</td>\n",
       "      <td>45750.0</td>\n",
       "      <td>black</td>\n",
       "      <td>black</td>\n",
       "      <td>29700.0</td>\n",
       "      <td>31700.0</td>\n",
       "      <td>2025-04-17 18:40:11.879574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_sales  year   brand_car transmission state condition  odometer  \\\n",
       "0           400  2009         BMW                 pa      33.0   58609.0   \n",
       "1             7  2012        Ford    automatic    il      32.0   39151.0   \n",
       "2          2514  2003      Toyota                 nc      36.0   83638.0   \n",
       "3          4469  2006                             nj      36.0   59574.0   \n",
       "4          4957  2000        Ford                 va       nan   88098.0   \n",
       "...         ...   ...         ...          ...   ...       ...       ...   \n",
       "29995     29996  2012        Ford    automatic    mi      36.0  103016.0   \n",
       "29996     29997  2012  Volkswagen    automatic    fl      36.0   41092.0   \n",
       "29997     29998  2003      Toyota                 az      19.0  292925.0   \n",
       "29998     29999  2014   Chevrolet    automatic    fl       4.0   25083.0   \n",
       "29999     30000  2012         RAM                 pa      41.0   45750.0   \n",
       "\n",
       "         color interior      mmr sellingprice                 created_at  \n",
       "0         blue      tan  17250.0      19000.0 2025-04-17 18:40:11.879574  \n",
       "1          red      tan   9525.0       9700.0 2025-04-17 18:40:11.879574  \n",
       "2        black     grey   3550.0       4000.0 2025-04-17 18:40:11.879574  \n",
       "3        white    black  12800.0       8000.0 2025-04-17 18:40:11.879574  \n",
       "4        black  unknown   1700.0       1900.0 2025-04-17 18:40:11.879574  \n",
       "...        ...      ...      ...          ...                        ...  \n",
       "29995    white     grey  17700.0      16800.0 2025-04-17 18:40:11.879574  \n",
       "29996  unknown      tan   8475.0       9800.0 2025-04-17 18:40:11.879574  \n",
       "29997    white     grey   3225.0       2400.0 2025-04-17 18:40:11.879574  \n",
       "29998     grey     grey  12900.0      12800.0 2025-04-17 18:40:11.879574  \n",
       "29999    black    black  29700.0      31700.0 2025-04-17 18:40:11.879574  \n",
       "\n",
       "[30000 rows x 12 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stg_car_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WAREHOUSE_Transform Car Sales** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_and_merge_categories(df):\n",
    "    # Mapping for merging similar categories in the 'color' column\n",
    "    color_mapping = {\n",
    "        '—': '', '': '', '16633': '', '6388': '', \n",
    "        'off-white': 'white', 'white': 'white', 'gray': 'grey'\n",
    "    }\n",
    "\n",
    "    # Mapping for merging similar categories in the 'interior' column\n",
    "    interior_mapping = {\n",
    "        '—': '', '': '', 'off-white': 'white', 'white': 'white', \n",
    "        'gray': 'grey', 'green': 'green'\n",
    "    }\n",
    "\n",
    "    # Apply the category merging mappings to the 'color' and 'interior' columns\n",
    "    df['color'] = df['color'].astype(str).str.lower().map(color_mapping).fillna(df['color'])\n",
    "    df['interior'] = df['interior'].astype(str).str.lower().map(interior_mapping).fillna(df['interior'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_invalid_values(df):\n",
    "    # Replace '' and '—' to np.nan and drop\n",
    "    df = df.replace({'': np.nan, '—': np.nan})\n",
    "\n",
    "    # Drop all row with NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Drop rows where 'condition' is NaN\n",
    "    df = df[df['condition'].notna()]\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_sales_by_id(df):\n",
    "    # Function to drop rows where 'id_sales' is 8013 or 26976\n",
    "    df = df[~df['id_sales'].isin([8013, 26976])]\n",
    "\n",
    "    # Drop rows with invalid state code\n",
    "    invalid_states = ['3vwd17aj5fm219943', '3vwd17aj5fm297123']\n",
    "    df = df[~df['state'].isin(invalid_states)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def mapping_target(df, df_car_brand, df_us_state):\n",
    "    # Map 'brand_car' from car_sales to 'brand_car_id' from car_brand table\n",
    "    brand_mapping = dict(zip(df_car_brand['brand_name'], df_car_brand['brand_car_id']))\n",
    "    df['brand_car_id'] = df['brand_car'].map(brand_mapping)\n",
    "\n",
    "    # Map 'state' from car_sales to 'id_state' from us_state table\n",
    "    state_mapping = dict(zip(df_us_state['code'], df_us_state['id_state']))\n",
    "    df['id_state'] = df['state'].map(state_mapping)\n",
    "\n",
    "    # Convert 'year' from varchar to int4\n",
    "    df['year'] = pd.to_numeric(df['year'], errors='coerce', downcast='integer')\n",
    "\n",
    "    # Convert 'condition', 'odometer', 'mmr', and 'sellingprice' from varchar to float4\n",
    "    df['condition'] = pd.to_numeric(df['condition'], errors='coerce', downcast='float')\n",
    "    df['odometer'] = pd.to_numeric(df['odometer'], errors='coerce', downcast='float')\n",
    "    df['mmr'] = pd.to_numeric(df['mmr'], errors='coerce', downcast='float')\n",
    "    df['sellingprice'] = pd.to_numeric(df['sellingprice'], errors='coerce', downcast='float')\n",
    "\n",
    "    # Rename columns to match the warehouse schema\n",
    "    df = df.rename(columns={\n",
    "        'id_sales': 'id_sales_nk',\n",
    "        'sellingprice': 'selling_price'\n",
    "    })\n",
    "\n",
    "    # Ensure all required columns are present and ordered correctly\n",
    "    warehouse_columns = [\n",
    "        'id_sales_nk',\n",
    "        'year',\n",
    "        'brand_car_id',\n",
    "        'transmission',\n",
    "        'id_state',\n",
    "        'condition',\n",
    "        'odometer',\n",
    "        'color',\n",
    "        'interior',\n",
    "        'mmr',\n",
    "        'selling_price',\n",
    "        'created_at'\n",
    "    ]\n",
    "\n",
    "    # Filter columns to match the warehouse schema\n",
    "    df = df[warehouse_columns]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_car_sales(df, df_car_brand, df_us_state):\n",
    "    # Step 1: Clean and merge categories\n",
    "    df = clean_and_merge_categories(df)\n",
    "    \n",
    "    # Step 2: Drop rows with invalid values\n",
    "    df = drop_invalid_values(df)\n",
    "    \n",
    "    # Step 3: Drop rows with 'id_sales' 8013 and 26976\n",
    "    df = drop_sales_by_id(df)\n",
    "\n",
    "    # Step 4: Mapping and transformation\n",
    "    df = mapping_target(df, df_car_brand, df_us_state)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WAREHOUSE_Load Warehouse** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helper.utils import get_db_connection, etl_log\n",
    "from datetime import datetime\n",
    "from pangres import upsert\n",
    "\n",
    "def load_warehouse(data, schema: str, table_name: str, idx_name: str):\n",
    "    try:\n",
    "        conn = get_db_connection('warehouse')\n",
    "        data = data.iloc[:, :-1] #Remove crated_at in the last column        \n",
    "        data = data.set_index(idx_name)\n",
    "\n",
    "        # Do upsert (Update for existing data and Insert for new data)\n",
    "        upsert(con=conn, \n",
    "               df=data, \n",
    "               table_name=table_name, \n",
    "               schema=schema, \n",
    "               if_row_exists=\"update\")\n",
    "        \n",
    "        #create success log message\n",
    "        log_msg = {\n",
    "            \"step\": \"warehouse\",\n",
    "            \"component\": \"load\",\n",
    "            \"status\": \"success\",\n",
    "            # \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\": \"warehouse\",\n",
    "            \"component\": \"load\",\n",
    "            \"status\": \"failed\",\n",
    "            # \"source\": source,\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        \n",
    "        # Handling error: save data to Object Storage\n",
    "        # try:\n",
    "        #     handle_error(data = data, bucket_name='error-paccar', table_name= table_name, step='warehouse', component='load')\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WAREHOUSE PIPELINE** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warehouse.extract.extract_stg import extract_staging\n",
    "from warehouse.transform.transform_car_sales import transform_car_sales\n",
    "from warehouse.load.load_wh import load_warehouse\n",
    "\n",
    "def warehouse_pipeline():\n",
    "    # Extract data from staging\n",
    "    stg_car_sales = extract_staging(\"car_sales\")\n",
    "    stg_us_state = extract_staging(\"us_state\")\n",
    "    stg_car_brand = extract_staging(\"car_brand\")\n",
    "\n",
    "    # Transform car sales data from staging\n",
    "    tf_stg_car_sales = transform_car_sales(\"stg_car_sales\")\n",
    "\n",
    "    # Load data into warehouse\n",
    "    load_warehouse(data=tf_stg_car_sales, schema='public', table_name='car_sales', idx_name='id_sales_nk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State code yang tidak ditemukan di mapping: ['3vwd17aj5fm219943' '3vwd17aj5fm297123']\n"
     ]
    }
   ],
   "source": [
    "missing_states = stg_car_sales[stg_car_sales['id_state'].isnull()]\n",
    "print(\"State code yang tidak ditemukan di mapping:\", missing_states['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.info of 0         9.0\n",
       "2        30.0\n",
       "3         9.0\n",
       "5        38.0\n",
       "6        13.0\n",
       "         ... \n",
       "29989    13.0\n",
       "29991     3.0\n",
       "29992    35.0\n",
       "29995    22.0\n",
       "29998     9.0\n",
       "Name: id_state, Length: 23999, dtype: float64>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_stg_car_sales['id_state'].info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. MODELLING** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODELLING_Extract Warehouse** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import text \n",
    "import sqlalchemy\n",
    "from helper.utils import get_db_connection, etl_log, read_etl_log, read_sql\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_warehouse(table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts all data from the warehouse database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection('warehouse')\n",
    "        \n",
    "        # Get the latest etl_date from the log table\n",
    "        filter_log = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"load\",\n",
    "            \"status\": \"success\",\n",
    "            \"table_name\": table_name\n",
    "        }\n",
    "        etl_date = read_etl_log(filter_log)\n",
    "\n",
    "        # If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "        # Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "        # Set etl_date for incremental extraction\n",
    "        if etl_date.empty or etl_date['latest_etl_date'][0] is None:\n",
    "            etl_date = '1111-01-01'\n",
    "        else:\n",
    "            etl_date = etl_date['latest_etl_date'][0]\n",
    "\n",
    "        # Constructs a SQL query to select all columns from the specified table_name\n",
    "        #  where created_at is greater than etl_date.\n",
    "        \"\"\"\n",
    "        SELECT * \n",
    "        FROM car_sales \n",
    "        \"\"\"\n",
    "        # WHERE created_at > :etl_date\n",
    "        query = sqlalchemy.text(read_sql(table_name))\n",
    "        # df = pd.read_sql(sql=query, con=conn, params={\"etl_date\": etl_date})\n",
    "        df = pd.read_sql(sql=query, con=conn, params={})\n",
    "\n",
    "        # Log success\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"extract_warehouse\",\n",
    "            \"status\": \"success\",\n",
    "            # \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"extract_warehouse\",\n",
    "            \"status\": \"failed\",\n",
    "            # \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        print(e)\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODELLING_Process Preprocessing** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from datetime import datetime\n",
    "from helper.utils import etl_log  # Make sure this utility function is available\n",
    "\n",
    "def process_preprocessing(df: pd.DataFrame, features: list, target: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform preprocessing: handle nulls, encoding, and scaling.\n",
    "\n",
    "    Steps:\n",
    "    1. Drop rows with '' or 'unknown' in features or target\n",
    "    2. Encode categorical columns\n",
    "    3. Scale numerical columns\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Drop rows containing '' or 'unknown'\n",
    "        # df = df[~df[features + [target]].isin(['', 'unknown']).any(axis=1)]\n",
    "        \n",
    "        # Log success message after cleaning missing data\n",
    "        # log_msg = {\n",
    "        #     \"step\": \"modelling\",\n",
    "        #     \"component\": \"preprocessing_drop_rows\",\n",
    "        #     \"status\": \"success\",\n",
    "        #     \"table_name\": \"car_sales\",\n",
    "        #     \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        # }\n",
    "        # etl_log(log_msg)\n",
    "\n",
    "        # Step 1: Encode categorical columns\n",
    "        categorical_cols = df[features].select_dtypes(include='object').columns.tolist()\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "        \n",
    "        # Log success message after encoding categorical features\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"preprocessing_encode_categorical\",\n",
    "            \"status\": \"success\",\n",
    "            \"table_name\": \"car_sales\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        etl_log(log_msg)\n",
    "\n",
    "        # Step 2: Scale numerical columns\n",
    "        numerical_cols = df[features].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        scaler = StandardScaler()\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # Log success message after scaling numerical features\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"preprocessing_scale_numerical\",\n",
    "            \"status\": \"success\",\n",
    "            \"table_name\": \"car_sales\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        etl_log(log_msg)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"preprocessing_process_preprocessing\",\n",
    "            \"status\": \"failed\",\n",
    "            \"table_name\": \"car_sales\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        etl_log(log_msg)\n",
    "        raise\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODELLING_Split Data** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from helper.utils import etl_log  # Make sure this utility function is available\n",
    "\n",
    "def split_data(df: pd.DataFrame, features: list, target: str, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset into train and test sets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X = df[features]\n",
    "        y = df[target]\n",
    "        \n",
    "        # Log success message before splitting\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"split_data\",\n",
    "            \"status\": \"success\",\n",
    "            \"table_name\": \"car_sales\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        etl_log(log_msg)\n",
    "        \n",
    "        return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"split_data\",\n",
    "            \"status\": \"failed\",\n",
    "            \"table_name\": \"car_sales\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        etl_log(log_msg)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import text \n",
    "import sqlalchemy\n",
    "from helper.utils import get_db_connection, etl_log, read_etl_log, read_sql\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_warehouse(table_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts all data from the warehouse database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection('warehouse')\n",
    "        \n",
    "        # Get the latest etl_date from the log table\n",
    "        filter_log = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"load\",\n",
    "            \"status\": \"success\",\n",
    "            \"table_name\": table_name\n",
    "        }\n",
    "        etl_date = read_etl_log(filter_log)\n",
    "\n",
    "        # If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "        # Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "        # Set etl_date for incremental extraction\n",
    "        if etl_date.empty or etl_date['latest_etl_date'][0] is None:\n",
    "            etl_date = '1111-01-01'\n",
    "        else:\n",
    "            etl_date = etl_date['latest_etl_date'][0]\n",
    "\n",
    "        # Constructs a SQL query to select all columns from the specified table_name\n",
    "        #  where created_at is greater than etl_date.\n",
    "        \"\"\"\n",
    "        SELECT * \n",
    "        FROM car_sales \n",
    "        \"\"\"\n",
    "        # WHERE created_at > :etl_date\n",
    "        query = sqlalchemy.text(read_sql(table_name))\n",
    "        # df = pd.read_sql(sql=query, con=conn, params={\"etl_date\": etl_date})\n",
    "        df = pd.read_sql(sql=query, con=conn, params={})\n",
    "\n",
    "        # Log success\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"extract_warehouse\",\n",
    "            \"status\": \"success\",\n",
    "            # \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"extract_warehouse\",\n",
    "            \"status\": \"failed\",\n",
    "            # \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "        print(e)\n",
    "    finally:\n",
    "        etl_log(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODELLING_Linear Regression Main Script** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1022.1468501632764\n",
      "MSE: 2470617.4329239563\n",
      "R²: 0.971674869947022\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime\n",
    "from helper.utils import etl_log\n",
    "from modelling.extract.extract_warehouse import extract_warehouse\n",
    "from modelling.preprocessing.preprocessing_data import process_preprocessing\n",
    "from modelling.preprocessing.splitting_data import split_data\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Extract data from warehouse database\n",
    "df = extract_warehouse(table_name=\"car_sales\")\n",
    "\n",
    "# Define features and target\n",
    "features = ['year', 'condition', 'odometer', 'mmr']\n",
    "target = 'selling_price'\n",
    "\n",
    "try:\n",
    "    # Step 1: Preprocess\n",
    "    df_processed = process_preprocessing(df, features, target)\n",
    "    df_processed = df_processed.dropna()\n",
    "\n",
    "    # Log success message after preprocessing\n",
    "    log_msg = {\n",
    "        \"step\": \"modelling\",\n",
    "        \"component\": \"preprocess_data\",\n",
    "        \"status\": \"success\",\n",
    "        \"table_name\": \"car_sales\",\n",
    "        \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    etl_log(log_msg)\n",
    "\n",
    "    # Step 2: Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df_processed, features, target)\n",
    "\n",
    "    # Log success message after splitting data\n",
    "    log_msg = {\n",
    "        \"step\": \"modelling\",\n",
    "        \"component\": \"split_data\",\n",
    "        \"status\": \"success\",\n",
    "        \"table_name\": \"car_sales\",\n",
    "        \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    etl_log(log_msg)\n",
    "\n",
    "    # Step 3: Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Log success message after training model\n",
    "    log_msg = {\n",
    "        \"step\": \"modelling\",\n",
    "        \"component\": \"train_model\",\n",
    "        \"status\": \"success\",\n",
    "        \"table_name\": \"car_sales\",\n",
    "        \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    etl_log(log_msg)\n",
    "\n",
    "    # Step 4: Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "    print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "    print(\"R²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "    # Log success message after evaluation\n",
    "    log_msg = {\n",
    "        \"step\": \"modelling\",\n",
    "        \"component\": \"evaluate_model\",\n",
    "        \"status\": \"success\",\n",
    "        \"table_name\": \"car_sales\",\n",
    "        \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    etl_log(log_msg)\n",
    "\n",
    "    # Step 5: Save model and dump to minIo\n",
    "    model_filename = \"carprice-model.pkl\"\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "    # Upload the model to MinIO\n",
    "    client = Minio('localhost:9001',\n",
    "                access_key=os.getenv('MINIO_ACCESS_KEY'),\n",
    "                secret_key=os.getenv('MINIO_SECRET_KEY'),\n",
    "                secure=False)\n",
    "    \n",
    "    # Make a bucket if it doesn't exist\n",
    "    bucket_name = \"car-sales-modelling\"\n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "\n",
    "    # Save the model to the MinIO bucket 'models'\n",
    "    client.fput_object(bucket_name, model_filename, model_filename)\n",
    "\n",
    "    # Log success message after saving model\n",
    "    log_msg = {\n",
    "        \"step\": \"modelling\",\n",
    "        \"component\": \"save_model\",\n",
    "        \"status\": \"success\",\n",
    "        \"table_name\": \"car_sales\",\n",
    "        \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    etl_log(log_msg)\n",
    "\n",
    "except Exception as e:\n",
    "    log_msg = {\n",
    "        \"step\": \"modelling\",\n",
    "        \"component\": \"linear_regression\",\n",
    "        \"status\": \"failed\",\n",
    "        \"table_name\": \"car_sales\",\n",
    "        \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"error_msg\": str(e)\n",
    "    }\n",
    "    etl_log(log_msg)\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
